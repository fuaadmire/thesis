2
2019-08-27 17:16:44.820296: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-08-27 17:16:45.443862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:83:00.0
totalMemory: 11.91GiB freeMemory: 11.77GiB
2019-08-27 17:16:45.443904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2019-08-27 17:16:45.898844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-08-27 17:16:45.898893: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2019-08-27 17:16:45.898901: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2019-08-27 17:16:45.899046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11387 MB memory) -> physical GPU (device: 0, name: TITAN X (Pascal), pci bus id: 0000:83:00.0, compute capability: 6.1)
2019-08-27 17:14:33.574616
trainingdata= BS
BS labels: 1=fake, 0=real
9822 4911
train seq shape (9822, 100)
(9822, 2)
Parameters:: num_cells: 256 num_samples: 9822 embedding_size: 300 epochs: 10 batch_size: 64
(?, 100)
fitting model..
Train on 9822 samples, validate on 4911 samples
Epoch 1/10
 - 46s - loss: 0.6146 - acc: 0.6612 - val_loss: 0.5189 - val_acc: 0.7677
Epoch 2/10
 - 43s - loss: 0.4728 - acc: 0.7765 - val_loss: 0.4390 - val_acc: 0.8088
Epoch 3/10
 - 43s - loss: 0.4138 - acc: 0.8112 - val_loss: 0.3924 - val_acc: 0.8302
Epoch 4/10
 - 44s - loss: 0.3663 - acc: 0.8426 - val_loss: 0.3674 - val_acc: 0.8491
Epoch 5/10
 - 44s - loss: 0.3270 - acc: 0.8606 - val_loss: 0.3469 - val_acc: 0.8534
Epoch 6/10
 - 44s - loss: 0.2961 - acc: 0.8794 - val_loss: 0.3272 - val_acc: 0.8621
Epoch 7/10
 - 44s - loss: 0.2569 - acc: 0.8993 - val_loss: 0.3138 - val_acc: 0.8668
Epoch 8/10
 - 44s - loss: 0.2288 - acc: 0.9080 - val_loss: 0.3067 - val_acc: 0.8699
Epoch 9/10
 - 44s - loss: 0.2050 - acc: 0.9223 - val_loss: 0.3111 - val_acc: 0.8703
Epoch 10/10
 - 44s - loss: 0.1768 - acc: 0.9318 - val_loss: 0.3017 - val_acc: 0.8770
Using TensorFlow backend.
[nltk_data] Downloading package punkt to /home/ktj250/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
Testing...
Test loss: 0.2927445269257664
Test accuracy: 0.8810803362106643
Valid loss: 0.3017371162071269
Valid accuracy: 0.8770107921721906
tn, fp, fn, tp
2798 430 433 3596
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input (InputLayer)           (None, 100)               0         
_________________________________________________________________
embedding_1 (Embedding)      (None, 100, 300)          14038200  
_________________________________________________________________
bidirectional_1 (Bidirection (None, 512)               1140736   
_________________________________________________________________
dense_1 (Dense)              (None, 2)                 1026      
=================================================================
Total params: 15,179,962
Trainable params: 15,179,962
Non-trainable params: 0
_________________________________________________________________
Testing on kaggle
Kaggle labels: 
 1: unreliable, 0: reliable
accuracy: 0.6718192627824019
F1= 0.6860961046346318
tn, fp, fn, tp
2107 1282 926 2413
Testing on FNC
FNC labels: 
 1: Fake, 0: Reliable
accuracy: 0.472969696969697
F1= 0.5270828801392213
tn, fp, fn, tp
2958 5219 3477 4846
Testing on BS
BS labels: 1=fake, 0=real
accuracy: 0.881080336227091
F1= 0.8928615766604594
tn, fp, fn, tp
2798 430 433 3596
Testing on Liar
Training with Fake= 1
Training with Fake= 1
Training with Fake= 1
accuracy: 0.44119968429360695
F1= 0.6084070796460177
tn, fp, fn, tp
9 705 3 550
accuracy: 0.48130841121495327
F1= 0.6472457627118644
tn, fp, fn, tp
7 661 5 611
